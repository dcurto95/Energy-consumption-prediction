{
  "test_name": "Changing_Optimizer",
  "tunning_parameter": {
    "from": "training",
    "name": "optimizer",
    "max_value": 0.5,
    "step": ["adam", "rmsprop", "sgd", "adamax", "adadelta", "adagrad"]
  },
  "arch": {
    "neurons": 32,
    "neurons_increase": 2,
    "rnn": "GRU",
    "drop": 0.0,
    "rnn_layers": 4,
    "dense_layers": 1,
    "activation": "relu",
    "activation_r": "hard_sigmoid",
    "window_size": 8
  },
  "training": {
    "batch": 1024,
    "epochs": 100,
    "optimizer": "adam",
    "lrate": 0.001
  }
}
